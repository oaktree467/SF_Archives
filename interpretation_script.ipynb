{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from calendar import monthrange\n",
    "from parsers.GridParser import GridParser\n",
    "from parsers.IssueParser import IssueParser\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "##USER INPUT: set test variable to true if you want to use dummy data instead of scraped data\n",
    "test = False\n",
    "##USER INPUT: Input URL of relevant Issue Grid from ISFDB here\n",
    "url = \"https://isfdb.org/cgi-bin/seriesgrid.cgi?20321\"\n",
    "\n",
    "##STEP 1: Read in URI Data\n",
    "df_URI = pd.read_excel(\n",
    "    ##USER INPUT: Enter Excel file here\n",
    "    io='MagazineURIs.xlsx',\n",
    "\n",
    "    ##USER INPUT: Enter Sheet Name Here (If file contains more than one sheet)\n",
    "    sheet_name ='SampleAsimov' \n",
    ")\n",
    "\n",
    "##USER INPUT\n",
    "##STEP 1A: Enter Resource EAD (What is the Identifier of the whole magazine?)\n",
    "resource_EAD = \"XY123 .Z34\"\n",
    "\n",
    "##Convert Year Month Format of Start and End Columns to yyyy-mm-dd\n",
    "#date = pd.to_datetime(df_URI['Start'])\n",
    "df_URI['Start'] = pd.to_datetime(df_URI['Start'])\n",
    "\n",
    "#date = pd.to_datetime(df_URI['End'])\n",
    "df_URI['End'] = pd.to_datetime(df_URI['End']) + pd.offsets.MonthEnd(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##STEP 2: Read in Inventory Grid\n",
    "df_Inventory = pd.read_excel(\n",
    "    ##USER INPUT: Excel File path here\n",
    "    io='MagazineInventory.xlsx',\n",
    "\n",
    "    ##USER INPUT: Enter Sheet Name Here (If file contains more than one sheet)\n",
    "    sheet_name ='SampleAsimov' \n",
    ")\n",
    "df_Inventory.index = df_Inventory['Year']\n",
    "del df_Inventory['Year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##STEP 3: Read in Issue Grid from ISFDB\n",
    "gridParser = GridParser(url, test)\n",
    "df_IssueGrid = pd.DataFrame(gridParser.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following Issues Not Found on ISFDB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Cleaning the Scraped Data\n",
    "df_IssueGrid['Year'] = (df_IssueGrid[\"Issue\"].str.split(\" \")).str[-1]\n",
    "\n",
    "#Don't split Mid-Month issue names (e.g, Mid-December)\n",
    "mask = df_IssueGrid['Issue'].str.replace(\"-\",\"/\")\n",
    "df_IssueGrid['Issue'] = df_IssueGrid['Issue'].where((df_IssueGrid['Issue'].str.contains('Mid')), mask)\n",
    "\n",
    "#Remove Issue Numbers\n",
    "df_IssueGrid['Issue'] = df_IssueGrid['Issue'].str.replace(\"^#\\d+\\s\", \"\", regex=True)\n",
    "\n",
    "#Split\n",
    "df_IssueGrid['Start'] = (df_IssueGrid[\"Issue\"].str.split(\" \")).str[0]\n",
    "\n",
    "try:\n",
    "    df_IssueGrid[[\"Start\", \"End\"]] = df_IssueGrid['Start'].str.split(\"/\",expand=True)\n",
    "except:\n",
    "    df_IssueGrid[\"End\"] = None\n",
    "\n",
    "for x in df_IssueGrid.index:\n",
    "    if df_IssueGrid.loc[x, \"End\"] is None:\n",
    "        df_IssueGrid.loc[x, \"End\"] = df_IssueGrid.loc[x, \"Start\"]\n",
    "\n",
    "#Reassigning the Issue Grid to a master dataframe\n",
    "df_master = df_IssueGrid.copy()\n",
    "\n",
    "df_master['Issue'] = df_master['Issue'].str.replace('\\d+','',regex=True)\n",
    "df_master['Issue'] = df_master['Issue'] + ' ' + df_master['Year']\n",
    "df_master['Year'] = pd.to_numeric(df_IssueGrid['Year'])\n",
    "df_master['Start'] = df_master['Start'].astype(str)\n",
    "df_master[\"In Inventory\"] = \"\"\n",
    "\n",
    "df_instances = []\n",
    "\n",
    "#Convert Excel Cells with number entries into series\n",
    "df_instances = df_Inventory.apply(lambda row: row[(row.notna())].index.tolist(), axis=1)\n",
    "\n",
    "\n",
    "#dataframe of entries reportedly in inventory, but not found on ISFDB\n",
    "notFoundList = []\n",
    "\n",
    "#Match the Excel entries 'x' with entries in the IssueGrid\n",
    "for x in df_instances.index: #year\n",
    "    for y in df_instances[x]: #month\n",
    "        if not (df_master.loc[((df_master['Year'] == x) & (df_master['Start'] == y))]).empty:\n",
    "            df_master.loc[((df_master['Year'] == x) & (df_master['Start'] == y)), \"In Inventory\"] = \"Yes\"\n",
    "            if ((df_Inventory.at[x, y]) > 1):\n",
    "                line = df_master.loc[((df_master['Year'] == x) & (df_master['Start'] == y))]\n",
    "                for i in range(int(df_Inventory.at[x, y]) - 1):\n",
    "                    df_master = pd.concat([df_master.iloc[:(line.index[0])], line, df_master.iloc[(line.index[0]):]])\n",
    "        else:\n",
    "            missingIssue = x,\" \",y\n",
    "            notFoundList.append(missingIssue)\n",
    "\n",
    "df_notFound = pd.DataFrame(notFoundList)\n",
    "print(\"Following Issues Not Found on ISFDB\")\n",
    "display(df_notFound)\n",
    "\n",
    "#reset dataframe so that only entries in inventory are present\n",
    "df_master = df_master[(df_master[\"In Inventory\"] == \"Yes\")]\n",
    "df_master = df_master.reset_index(drop=True)\n",
    "df_master = df_master.drop(columns=[\"In Inventory\"])\n",
    "df_master[\"Volume/Number\"] = \"\"\n",
    "\n",
    "df_master['End'] = df_master['End'].str.replace(\"Mid-\",\"\")\n",
    "mask = df_master['Start'].str.replace(\"Mid-\",\"15 \")\n",
    "df_master['Start'] = df_master['Start'].mask(df_master['Start'].str.contains(\"Mid-\"),mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Scrape Volume and Number Data from individual ISFDB Pages\n",
    "\n",
    "parser = IssueParser(test)\n",
    "for x in df_master[\"URL\"]:\n",
    "    li = parser.run(x)\n",
    "    try:\n",
    "        vol = \"Vol. \" + li[1] + \", No. \" + li[3]\n",
    "    except:\n",
    "        vol = \"\"\n",
    "    df_master.loc[(df_master[\"URL\"] == x), \"Volume/Number\"] = vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Clean up Volume/Number Column\n",
    "df_master[\"Volume/Number\"] = df_master[\"Volume/Number\"].str.replace(\",,\",\",\")\n",
    "\n",
    "#Normalize Magazine Date for Comparison Purposes\n",
    "#Two blocks following: if issue is a season\n",
    "df_master.loc[(df_master[\"Start\"] == \"Spring\"), \"Start\"] = \"January\"\n",
    "df_master.loc[(df_master[\"Start\"] == \"Summer\"), \"Start\"] = \"April\"\n",
    "df_master.loc[(df_master[\"Start\"] == \"Fall\"), \"Start\"] = \"July\"\n",
    "df_master.loc[(df_master[\"Start\"] == \"Winter\"), \"Start\"] = \"October\"\n",
    "\n",
    "df_master.loc[(df_master[\"End\"] == \"Spring\"), \"End\"] = \"March\"\n",
    "df_master.loc[(df_master[\"End\"] == \"Summer\"), \"End\"] = \"June\"\n",
    "df_master.loc[(df_master[\"End\"] == \"Fall\"), \"End\"] = \"September\"\n",
    "df_master.loc[(df_master[\"End\"] == \"Winter\"), \"End\"] = \"December\"\n",
    "\n",
    "df_master[\"interval\"] = df_master[\"Start\"] + \" \" + df_master[\"Year\"].astype(str)\n",
    "df_master[\"interval\"] = pd.to_datetime(df_master[\"interval\"])\n",
    "df_master[\"interval_end\"] = df_master[\"End\"] + \" \" + df_master[\"Year\"].astype(str)\n",
    "df_master[\"interval_end\"] = pd.to_datetime(df_master[\"interval_end\"]) + pd.offsets.MonthEnd(n=1)\n",
    "df_URI[\"interval\"] = pd.to_datetime(df_URI[\"Start\"])\n",
    "\n",
    "#sort data for merge\n",
    "df_master = df_master.sort_values(by=['interval'])\n",
    "df_URI = df_URI.sort_values(by=['interval'])\n",
    "\n",
    "#merge df_master and df_URI together\n",
    "df_master = pd.merge_asof(df_master, df_URI[['interval','URI']], on='interval')\n",
    "\n",
    "#Add in the EAD of the Resource\n",
    "df_master['Resource EAD'] = resource_EAD\n",
    "\n",
    "##STEP 4: Read in ArchivesSpace Template\n",
    "df_upload = pd.read_excel(\n",
    "    ##USER INPUT: Enter path to Excel file Template here\n",
    "    io='./bulk_import_template.xlsx',\n",
    "    sheet_name ='Data'  \n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'sfURL': df_master['URL'],\n",
    "    'Resource URI': df_master['URI'],\n",
    "    'EAD ID': df_master['Resource EAD'],\n",
    "    'Title': df_master['Volume/Number'],\n",
    "    'Hierarchical Relationship': 2,\n",
    "    'Description Level': 'Other Level',\n",
    "    'Other Level': 'Volume',\n",
    "    'Publish?': True,\n",
    "    'Language': 'English',\n",
    "    'Date(1) Label': 'Other',\n",
    "    'Date(1) Begin': df_master['interval'],\n",
    "    'Date(1) end': df_master['interval_end'],\n",
    "    'Date(1) Type': 'inclusive',\n",
    "    'Date(1) expression': df_master['Issue'].str[-4:] + ' ' + df_master['Issue'].str[:-4]\n",
    "    })\n",
    "\n",
    "df_upload = pd.concat([df_upload, df], ignore_index = True)\n",
    "\n",
    "for i in range(0, len(df_upload)):\n",
    "    if type(df_upload.loc[i, 'Date(1) Begin']) is not str:\n",
    "        df_upload.loc[i, 'Date(1) Begin'] = df_upload.loc[i, 'Date(1) Begin'].strftime('%Y-%m-%d')\n",
    "    if type(df_upload.loc[i, 'Date(1) end']) is not str:\n",
    "        df_upload.loc[i, 'Date(1) end'] = df_upload.loc[i, 'Date(1) end'].strftime('%Y-%m-%d')\n",
    "\n",
    "#Move the URL column to the front of the spreadsheet\n",
    "df_temp = df_upload.pop('sfURL')\n",
    "df_upload.insert(4, 'isfdbURL', df_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use this block to create one big sheet for every URI\n",
    "#--------------------------------------------------------\n",
    "df_upload.to_excel(\"output.xlsx\", index=False)\n",
    "#--------------------------------------------------------\n",
    "#Make any necessary revisions in the output excel file\n",
    "#Before running the next block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this block to create separate sheets for every URI\n",
    "#--------------------------------------------------------\n",
    "##STEP 1: Read in URI Data\n",
    "df_upload = pd.read_excel(\n",
    "    ##USER INPUT: Enter Excel file here\n",
    "    io='output.xlsx',\n",
    "    header=[0]\n",
    ")\n",
    "\n",
    "temp_headers = df_upload[0:4]\n",
    "df_upload = df_upload.drop([0,1,2,3])\n",
    "df_upload = df_upload.reset_index(drop=True)\n",
    "df_upload = df_upload.drop('isfdbURL',axis=1)\n",
    "#print_full(df_upload[['Resource URI','Title','Date(1) expression']])\n",
    "\n",
    "dataframe_collection = {}\n",
    "uri_Unique = df_upload.groupby(df_upload['Resource URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of dataframes, where each dataframe has only one Resource URI\n",
    "for x in uri_Unique.groups.keys():\n",
    "    newKey = df_URI.loc[(df_URI['URI']==x), 'Collection'].iloc[0]\n",
    "    dataframe_collection[newKey] = uri_Unique.get_group(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27140\\3754896374.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_collection[x]['Resource URI'] = \"\"\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27140\\3754896374.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_collection[x]['Resource URI'] = \"\"\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27140\\3754896374.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_collection[x]['Resource URI'] = \"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Export each dataframe as an Excel sheet with the name of the appropriate collection in ASpace\n",
    "for x in dataframe_collection:\n",
    "    dataframe_collection[x]['Resource URI'] = \"\"\n",
    "    file_name = str(x)\n",
    "    if (\"/\" in file_name):\n",
    "        file_name = file_name.replace(\"/\",\"\")\n",
    "    char = 0\n",
    "    while (file_name[char] == \" \"):\n",
    "        file_name = file_name[:char] + file_name[(char + 1):]\n",
    "        char += 1\n",
    "    #(dataframe_collection[x]).loc['Resource URI'] = \"\"\n",
    "    dataframe_collection[x] = pd.concat([dataframe_collection[x][:0], temp_headers, dataframe_collection[x][0:]]).reset_index(drop=True)\n",
    "    path = \"./xlsx_output/\" + file_name + \".xlsx\"\n",
    "    #File path:\n",
    "    dataframe_collection[x].to_excel(path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
